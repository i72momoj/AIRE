{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617aa042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#                 QUANTIZACIÓN MODELO ONNX                      #\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = './onnx/model.onnx'\n",
    "model_quant = './onnx/model.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268acb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#              TESTEO DEL MODELO QUANTIZADO ONNX                #\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "from pprint import pp\n",
    "\n",
    "# cargamos el modelo y el tokenizer del model cuantizado\n",
    "session = ort.InferenceSession(\"./onnx/model.quant.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./onnx\")\n",
    "\n",
    "# Extraemos los comandos para el testeo\n",
    "text = \"avísame en una hora y tres cuartos\"\n",
    "\n",
    "# Los tokenizamos\n",
    "tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"np\")  # Convert to NumPy array\n",
    "#tokens = tokenizer(text, return_tensors=\"np\")  # Convert to NumPy array\n",
    "\n",
    "\n",
    "# Ensure input tensor shape matches what the model expects\n",
    "input_ids = tokens[\"input_ids\"].astype(np.int64)  # ONNX models often require int64 inputs\n",
    "attention_mask = tokens[\"attention_mask\"].astype(np.int64)\n",
    "\n",
    "# Preparamos los inputs para pasárselos al modelo\n",
    "inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "}\n",
    "\n",
    "# Ejecutamos las predicciones y obtenemos los resultados\n",
    "outputs = session.run(None, inputs)\n",
    "\n",
    "#print(f\"\\nOUTPUTS: \\n{outputs}\")\n",
    "\n",
    "logits = torch.tensor(outputs[0])  # Convert output to a PyTorch tensor\n",
    "predictions = torch.argmax(logits, dim=-1)  # Get predicted class index\n",
    "'''predicted_classes = predictions.tolist()\n",
    "print(f\"Predicted classes: {predicted_classes}\")'''\n",
    "\n",
    "# Print tokens and predictions\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "pp(torch.argmax(logits, dim=-1))\n",
    "\n",
    "for token, label_id in zip(tokens, predictions[0].tolist()):\n",
    "    print(f\"{token}: {id2tag[label_id]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
