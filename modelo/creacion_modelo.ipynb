{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#                   Preparamos el Dataset                       #\n",
    "#---------------------------------------------------------------#\n",
    "import pandas as pd\n",
    "from datasets import Dataset, ClassLabel, DatasetDict, Sequence\n",
    "from dataset import datos\n",
    "\n",
    "#train_csv = \"./train.csv\"\n",
    "tags = ['O', 'B-DSEM', 'I-DSEM','B-HORA', 'I-HORA', 'B-MIN', 'I-MIN', 'B-RELH', 'I-RELH', 'B-RELM', 'I-RELM', 'B-RELS', 'I-RELS', 'B-MTN', 'I-MTN', 'B-TIT', \"I-TIT\"]\n",
    "\"\"\"Tags para las entidades a reconocer\n",
    "\n",
    "    Elementos\n",
    "    ----------\n",
    "        \n",
    "        * 'O': no es entidad\n",
    "        * 'DSEM': día de la semana\n",
    "        * 'HORA': hora del día\n",
    "        * 'MIN': minutos dentro de la hora\n",
    "        * 'REL': tiempo relativo a la hora actual (una hora, 30 minutos, etc.)\n",
    "        * 'MTN': Mañana/Tarde/Noche, si no se especifica por defecto será PM para las [1:00->7:00) y AM para las [7:00->12:00] \n",
    "        * 'TIT': título\n",
    "\"\"\"\n",
    "\n",
    "id2tag = {id: tag for id, tag in enumerate(tags)}\n",
    "'''Diccionario de IDs a sus correspondientes etiquetas de entidades'''\n",
    "\n",
    "tag2id = {tag: id for id, tag in enumerate(tags)}\n",
    "'''Diccionario de etiquetas de entidades a sus correspondientes IDs'''\n",
    "\n",
    "print('IMPORTANDO LOS DATASETS...')\n",
    "\n",
    "# Leemos el fichero con los datos de entrenamiento y testeo\n",
    "#df = pd.read_csv(train_csv)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Transformamos los datos del dataset para obtener las columnas\n",
    "# que nos interesan. En este caso serían los comandos separados\n",
    "# por palabras (tokens), las etiquetas de las entidades en formato\n",
    "# numérico (ner_tags) y en cadena de texto (ner_tags_str)\n",
    "\n",
    "# DESDE EL FICHERO dataset.py:\n",
    "df['tokens'] = datos['comandos']\n",
    "df['ner_tags_str'] = datos['tokens']\n",
    "\n",
    "df['ner_tags'] = df['ner_tags_str'].map(lambda lista: list(map(lambda tag: tag2id[tag], lista)))\n",
    "\n",
    "\n",
    "# Construímos el Dataset a partir de las 3 columnas anteriores y convertimos la columna \n",
    "# de ner_tags a un ClassLabel para mayor facilidad para el procesamiento\n",
    "labels=ClassLabel(names=tags)\n",
    "data = Dataset.from_pandas(df[['tokens','ner_tags_str','ner_tags']])\n",
    "data = data.cast_column(\"ner_tags\", Sequence(feature=labels))\n",
    "\n",
    "# Separamos el Dataset en dos distintos; uno para entrenamiento y otro para testeo,\n",
    "# y los agrupamos en un diccionario de Datasets\n",
    "train_dataset, test_dataset = data.train_test_split(test_size=0.01, shuffle=False).values()\n",
    "data = DatasetDict({'train': train_dataset, 'test': test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#                           Tokenizamos                         #\n",
    "#---------------------------------------------------------------#\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print('TOKENIZANDO INPUTS...')\n",
    "\n",
    "nombre_modelo = 'dccuchile/distilbert-base-spanish-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(nombre_modelo)\n",
    "\n",
    "def alinear(ids_palabras_tokenizadas, ner_tags_originales):\n",
    "    \"\"\"\n",
    "        Alinea los ner_tags originales, es decir las etiquetas BIO numeradas correspondientes\n",
    "        a los tokens originales, con los nuevos tokens resultantes de tokenizar las palabras originales\n",
    "        mediante el modelo preentrenado.\n",
    "\n",
    "        Devuelve\n",
    "        --------\n",
    "\n",
    "        new_tags\n",
    "            Nueva lista de ner_tags alineada con los nuevos tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    new_tags = list(\n",
    "        map(\n",
    "            lambda x: -100 if x == None else ner_tags_originales[x],\n",
    "            ids_palabras_tokenizadas\n",
    "        )\n",
    "    )\n",
    "    return new_tags\n",
    "\n",
    "def tokenizar_y_alinear(dataset):\n",
    "    \"\"\"\n",
    "    Tokeniza las filas del dataset pasado por parámetro, obtiene las NER tags\n",
    "    correspondientes a cada fila y las alinea con los tokens resultantes de la\n",
    "    tokenización.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    dataset\n",
    "        Dataset a tokenizar y alinear\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    inputs_tokenizados\n",
    "        Dataset pasado por parámetro, ya tokenizado y con las NER tags alineadas\n",
    "        con los nuevos tokens resultantes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenizamos todas las filas del dataset\n",
    "    inputs_tokenizados = tokenizer(dataset['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    # Obtenemos los IDs de las palabras tokenizadas y las NER tags originales\n",
    "    ner_tags_originales = dataset['ner_tags']\n",
    "    nfilas = len(inputs_tokenizados['input_ids'])\n",
    "    word_ids = list(inputs_tokenizados.word_ids(i) for i in range(nfilas))\n",
    "\n",
    "    # Alineamos las NER tags originales con los IDs de las palabras tokenizadas\n",
    "    new_labels = list(map(alinear, word_ids, ner_tags_originales))\n",
    "\n",
    "    # Creamos una nueva columna en el dataset con las tags alineadas\n",
    "    inputs_tokenizados['labels'] = new_labels\n",
    "\n",
    "    return inputs_tokenizados\n",
    "\n",
    "# Tokenizamos los datos mediante el tokenizer del modelo preentrenado\n",
    "tokenized_data = data.map(tokenizar_y_alinear, batched=True, remove_columns=data['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#                        Data collation                         #\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# El data collator lo único que hace es hacer todos los inputs del mismo tamaño y\n",
    "# añadir padding cuando es necesario a cada uno de los features de los datasets.\n",
    "# Aquí solamente lo invocamos, puesto que más tarde se le pasa al trainer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#                        Evaluación                             #\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metrica = evaluate.load('seqeval')\n",
    "\n",
    "def evaluar_metricas(predicciones_eval):\n",
    "\n",
    "    logits, etiquetas = predicciones_eval\n",
    "\n",
    "    predicciones = np.argmax(logits, axis=-1)\n",
    "\n",
    "    etiquetas_true = [[id2tag[e] for e in etiqueta if e != -100] for etiqueta in etiquetas]\n",
    "    predicciones_true = [ [id2tag[p] for p, e in zip(prediccion, etiqueta) if e!=-100] for prediccion, etiqueta in zip(predicciones, etiquetas)]\n",
    "\n",
    "    metricas = metrica.compute(predictions=predicciones_true, references=etiquetas_true)\n",
    "\n",
    "    return {\n",
    "        \"precision\": metricas['overall_precision'],\n",
    "        \"recall\": metricas['overall_recall'],\n",
    "        \"f1\": metricas['overall_f1'],\n",
    "        \"accuracy\": metricas['overall_accuracy']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#                  Parámetros de entrenamiento                  #\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from torch import device\n",
    "\n",
    "# Cargamos el modelo preentrenado\n",
    "modelo = AutoModelForTokenClassification.from_pretrained(\n",
    "    nombre_modelo,\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id\n",
    ")\n",
    "\n",
    "# Establecemos los parámetros de entrenamiento\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"finetuned-ner\", # Nombre del nuevo modelo\n",
    "    #eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit = 1,\n",
    "    #load_best_model_at_end=True,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Invocamos un \"entrenador\", le pasamos el modelo preentrenado, los\n",
    "# parámetros de entrenamiento, el dataset, el collator, la\n",
    "# función de evaluación y el tokenizador\n",
    "trainer = Trainer(\n",
    "    model=modelo,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=evaluar_metricas,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#                        Entrenamiento                          #\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "import torch\n",
    "\n",
    "print('COMENZANDO ENTRENAMIENTO...')\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "# Comenzamos a entrenar\n",
    "trainer.train()\n",
    "\n",
    "# Guardamos el modelo refinado\n",
    "torch.save(trainer.model, \"./modelo.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681dac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------#\n",
    "#                        Testeo                                 #\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "from transformers import pipeline\n",
    "from pprint import pp\n",
    "import torch\n",
    "\n",
    "frase = \"pon una alarma mañana a las diez\"\n",
    "\n",
    "# Debemos cambiarlo al nombre del checkpoint guardado en la carpeta\n",
    "modelo_checkpoint = \"finetuned-ner/checkpoint-22265\"\n",
    "\n",
    "# Cargamos el modelo refinado\n",
    "clasificador = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=modelo_checkpoint,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Inferimos\n",
    "pp(clasificador(frase))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
